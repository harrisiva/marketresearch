{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da31ea6",
   "metadata": {},
   "source": [
    "# Jiu-Jitsu in Americas\n",
    "The goal of this project is to find all jiu-jitsu academies primarily across north america in order to analyze the information and devise a multi-channel sales prospecting process for Keystone Kimonos.\n",
    "\n",
    "To acomplish this goal, the idea is to create a dataframe of jiu-jitsu academies, that can be offloaded and onloaded onto this python script. The dataframe will hold items scraped from online pages with the following attributes: academy name, address, province, country,timmings, phone, email, website.\n",
    "\n",
    "We can also employ data visualization tools empowered by text retrival and search engine tools to rank and derive knowledge from the collect data in dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b441432",
   "metadata": {},
   "source": [
    "## Setting Up Dataframe\n",
    "The objective in this section is to construct the set of functions responsible for downloading/saving and loading the dataframe into memory for computation. In addition to this, we extract the required information that is possible to scrape headless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a264fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# Extract the links of different club's smoothcomp page link from the given URL\n",
    "clubs = []\n",
    "\n",
    "try:\n",
    "    for i in range(0, 3000):\n",
    "        # Request the landing page from smoothcomp\n",
    "        page_number = i \n",
    "        response = requests.get(f\"https://smoothcomp.com/en/club?search=&country=&continent=North%20America&page={page_number}\")\n",
    "        soup = bs(response.text, 'html.parser')\n",
    "        # Extract the club's links from the current page along (discard all other information as they can be obtained from the next page)\n",
    "        for item in soup.find_all('a'):\n",
    "            _class = item.get(\"class\")\n",
    "            if _class and \"color-inherit\" in _class: \n",
    "                link = item.get('href')\n",
    "                clubs.append(link)\n",
    "        i+=1 # Increment the page number to capture a large set of clubs url's\n",
    "        if i%10==0: print(f\"\\tScraped {i}th page\")\n",
    "except:\n",
    "    print(f\"Stopped/Crashed at page number:{i}.\")\n",
    "\n",
    "print(f\"Acquired smoothcomp profiles of {len(clubs)} academies.\")\n",
    "\n",
    "print(\"Extracting club info...\")\n",
    "data = []\n",
    "try:\n",
    "    # Scrape information about each club from their smoothcomp profile to collect the required attributes information\n",
    "    for i in range(0, len(clubs)):\n",
    "        # Initialize dictionary for containing the current rows data (None as default when data not found)\n",
    "        row = {\"Name\":None, \"Location\":None,\"Contact Persons\":None, \"Affiliation\":None} \n",
    "        # Extract information for the current club and pack into the dictionary if data exists\n",
    "        response = requests.get(clubs[i])\n",
    "        soup = bs(response.text, 'html.parser')\n",
    "        row[\"Source\"] = clubs[i]\n",
    "        row[\"Name\"] = soup.title.text.replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"-Smoothcomp\",\"\")\n",
    "        club_info = [i for i in soup.find_all(id=\"clubInfo\")[0].text.split('\\n') if i]\n",
    "        if \"Location\" in club_info: row[\"Location\"] = club_info[int(club_info.index(\"Location\")+1)]\n",
    "        if \"Contact Persons\" in club_info: row[\"Contact Persons\"] = club_info[int(club_info.index(\"Contact Persons\")+1)]\n",
    "        if \"Affiliation\" in club_info: row[\"Affiliation\"] = club_info[int(club_info.index(\"Affiliation\")+1)]\n",
    "        # Append dictionary into the list of rows to convert into a pandas dataframe to work with\n",
    "        data.append(row)\n",
    "        i+=1 # Increment for next club\n",
    "        if i%10==0:print(f\"\\ttScraped {i}th club\")\n",
    "except:\n",
    "    print(f\"\\ttCompleted scrapping {i} clubs\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"ExtractedData.csv\", encoding='utf-8', index=False) \n",
    "print(f\"\\n\\nSaved csv with data on {len(df)} academies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3e818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
